\documentclass[11 pt, a4paper]{article}  % list options between brackets
\usepackage[nottoc,notlof,notlot,numbib]{tocbibind}
\usepackage[margin=1.2 in]{geometry}
\usepackage{fancyhdr}
\usepackage{manfnt}
\usepackage{pgf}
\usepackage{amsmath,amssymb,natbib,graphicx}
\usepackage{amsfonts}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{float}
\usepackage{mathrsfs} %mathscr{A}
\usepackage{subfigure}
\usepackage{chngpage}

\newtheorem{axiom}{Axiom}[section]
\newtheorem{result}{Result}[section]
\newtheorem{example}{example}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{principle}{Principle}[section]
\newtheorem{theorem}{Theorem}[section]% list packages between braces

% type user-defined commands here

\begin{document}

\title{Summary of Short-term Research Objectives}   % type title between braces
\author{Detian Deng}         % type author(s) between braces
\date{\today}    % type date between braces
\maketitle

%\begin{abstract}
%\end{abstract}

\section{Model Specification}             % section 1
Let $L$ be a K-dimensional Bernoulli random variable denoting the true state.
Consider the general log linear model:
\begin{align*}
f(l; \Theta) = & \exp \{\Theta_1^T l + \Theta_2^{T} u_2 + \ldots + \Theta_K^T u_K - A(\Theta)\}
\end{align*}
where $U_k$ is a ${K \choose k} \times 1$ vector of k-way cross-products, $k = 1,\ldots,K$,  and $\Theta = (\Theta_1,\ldots, \Theta_K)$ contains the the natural parameters, which is a $(2^K-1) \times 1$ vector.\\
\ \\
For the prior, let $S = \sum_{j=1}^K L_j = s$ has some fixed pmf $\pi(s)$ , $s = 0,1,\ldots, K$, and let each possible outcome of $L$ given $S = s$ occur with probability $\frac{\pi(s)}{{K \choose s}}.$\\

\section{Research Objectives}
\begin{enumerate}
\item Given the above model specification, what is the implied prior for $\Theta$? Try using Stick Braking prior as an example.
\item As a special case of the above model, if $\Theta_d,\Theta_{d+1},\ldots,\Theta_K$ are set to be $0$, for some $d$,  what is the implied prior for $\Theta$? Especially, by setting $d=3$, we have the quadratic exponential family model. 
\item Find a good way to visualize the marginal probability of all possible cases together.
\item Using the implied prior calculated above, simulate measurement data $Y$ from the following hierarchical model:
\begin{align*}
\Theta \sim & [\Theta | \pi, K]\\
L \sim & [L | \Theta]\\
Y \sim & [Y | L]
\end{align*}
where $[Y | L]$ is defined by $Pr(Y_j=1|L_j=1) = \psi_j$ and $Pr(Y_j=0|L_j=0) = \phi_j$ with $Y_j$ being conditional independent with each other given $L$.
\item Based on the data simulated above, estimate the individual etiology $P(L_i |Y)$ and the population etiology $P(\Theta |Y)$.
\end{enumerate}

\newpage
\section{Results}
\subsection{The Implied Prior}
\subsubsection{Log linear Model}
Let $A =\sum\exp\{\Theta_1^T l + \Theta_2^{T} u_2 + \ldots + \Theta_K^T u_K\}$. Since we assume each type of event given $s$ has the same probability, all elements in $\Theta_k$ are equal to $\theta_k$ for $k =1,\ldots,K$. Then we have the following equations: $\frac{1}{A} = \pi_0$ and 
\begin{align*}
{K \choose s}A^{-1} \exp \left \{ s\theta_1 + 1(s\geq 2) {s \choose 2} \theta_2 + \ldots + 1(s\geq K) {s \choose K} \theta_K \right \} = & \pi_s \text{ , for }s = 1,2,\ldots,K
\end{align*} 
By expanding the equation above, we have the following linear system:
\begin{align*}
\begin{pmatrix}
1 & 0 & 0 & \cdots & 0 &0  & 0\\ 
\vdots &  &  &  &  &  & \\ 
s & {s \choose 2} & \cdots &{s \choose s} &0  &\cdots &0 \\ 
 \vdots&  &  &  &  &  & \\ 
 K & {K \choose 2} & \cdots  &  & \cdots &  & {K \choose K}
\end{pmatrix}
\begin{pmatrix}
\theta_1\\ 
\vdots\\ 
\theta_s\\ 
\vdots\\ 
\theta_K
\end{pmatrix} & = 
\begin{pmatrix}
\log \frac{\pi_1}{\pi_0 K}\\ 
\vdots\\ 
\log \frac{\pi_s}{\pi_0 {K \choose s}}\\ 
\vdots\\ 
\log \frac{1-\pi_0-\ldots-\pi_{K-1}}{\pi_0}
\end{pmatrix}
\end{align*}
The inductive solution is:
\begin{align*}
\theta_1 = & \log \frac{\pi_1}{\pi_0 K}\\
\theta_s = & \log \frac{\pi_s}{\pi_0 {K \choose s}} - \sum_{j=1}^{s-1}{s \choose j}\theta_j
\end{align*}

\newpage
\subsubsection{Quadratic Exponential Family}
\begin{align*}
f(l; \Theta) = & \exp \{\Theta_a^T l + \Theta_b^{T} u - A(\Theta)\}
\end{align*}
where $U = (L_{1}L_{2}, \ldots, L_{K-1}L_{K})$ is a $K(K-1)/2 \times 1$ vector of two-way cross-products and $\Theta = (\Theta_a, \Theta_b)$ contains the the natural parameters, which is a $K(K+1)/2 \times 1$ vector.\\

Let $A =\sum\exp\{\Theta_a^T l + \Theta_b^{T} u\}$. Since we assume each type of event given $s$ has the same probability, all elements in $\Theta_a$ are equal to $\beta$, and all elements in $\Theta_b$ are equal to $\gamma$. When $s = 0, 1$, we have 
\begin{align*}
\frac{1}{A} = & \pi_0\\
\frac{\exp\{\beta\}}{A} = & \frac{\pi_1}{K}
\end{align*}
Then $\beta = \log \frac{\pi_1}{\pi_0 K}$.\\

For $s\geq 2$, we have
\begin{align*}
\exp\{s\beta + {s \choose 2}\gamma\}/A = & \pi_s/{K \choose s}\\
\pi_s = & \pi_0 {K \choose s} \exp\{s\beta + {s \choose 2}\gamma\}
\end{align*}
Since all $\pi_s$ should sum up to 1, we can solve the following equation for $\gamma$
\[1 = \pi_0 +\pi_1 + \sum_{s=2}^K \left[ \pi_0 {K \choose s} \exp\{s\beta + {s \choose 2}\gamma\}\right]\]
Hence $\beta,\gamma$, and $(\pi_2,\ldots,\pi_K)$ are determined given $\pi_0$, $\pi_1$ and $K$. 

%\begin{table}[htbp]
%\begin{adjustwidth}{-0.5in}{-1in}
%\resizebox{0.7\textwidth}{!}{\begin{minipage}{\textwidth}
%\caption{Summary of the model coefficients and standard error estimates}
%\label{tab: coef}
%
%\begin{tabular}{llllll|lllll} 
%\hline 
% \\
%\hline \\
%\end{tabular}
%\end{minipage}}
%\end{adjustwidth}
%\end{table}


\end{document}